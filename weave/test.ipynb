{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting weave\n",
      "  Downloading weave-0.51.17-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting emoji>=2.12.1 (from weave)\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting gql[aiohttp,requests] (from weave)\n",
      "  Downloading gql-3.5.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: numpy>1.21.0 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from weave) (2.1.1)\n",
      "Requirement already satisfied: packaging>=21.0 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from weave) (24.1)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from weave) (2.9.1)\n",
      "Collecting rich (from weave)\n",
      "  Downloading rich-13.9.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity!=8.4.0,>=8.3.0 (from weave)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting uuid-utils>=0.9.0 (from weave)\n",
      "  Downloading uuid_utils-0.9.0-cp312-cp312-macosx_10_12_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting wandb>=0.17.1 (from weave)\n",
      "  Downloading wandb-0.18.5-py3-none-macosx_11_0_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (2.23.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from pydantic>=2.0.0->weave) (4.12.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb>=0.17.1->weave)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.17.1->weave)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (5.28.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (6.0.0)\n",
      "Collecting pyyaml (from wandb>=0.17.1->weave)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.17.1->weave)\n",
      "  Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting setproctitle (from wandb>=0.17.1->weave)\n",
      "  Downloading setproctitle-1.3.3-cp312-cp312-macosx_10_9_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from wandb>=0.17.1->weave) (73.0.1)\n",
      "Collecting graphql-core<3.3,>=3.2 (from gql[aiohttp,requests]->weave)\n",
      "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.6 (from gql[aiohttp,requests]->weave)\n",
      "  Downloading yarl-1.16.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (63 kB)\n",
      "Collecting backoff<3.0,>=1.11.1 (from gql[aiohttp,requests]->weave)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting anyio<5,>=3.0 (from gql[aiohttp,requests]->weave)\n",
      "  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting aiohttp<4,>=3.9.0b0 (from gql[aiohttp,requests]->weave)\n",
      "  Downloading aiohttp-3.10.10-cp312-cp312-macosx_10_9_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting requests-toolbelt<2,>=1.0.0 (from gql[aiohttp,requests]->weave)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->weave)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from rich->weave) (2.18.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave)\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.9.0b0->gql[aiohttp,requests]->weave)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from anyio<5,>=3.0->gql[aiohttp,requests]->weave) (3.10)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.0->gql[aiohttp,requests]->weave)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb>=0.17.1->weave) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->weave)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hugoromero/anaconda3/envs/autodoc/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb>=0.17.1->weave) (2024.8.30)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.6->gql[aiohttp,requests]->weave)\n",
      "  Downloading propcache-0.2.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.1->weave)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading weave-0.51.17-py3-none-any.whl (266 kB)\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading uuid_utils-0.9.0-cp312-cp312-macosx_10_12_x86_64.whl (270 kB)\n",
      "Downloading wandb-0.18.5-py3-none-macosx_11_0_x86_64.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.3-py3-none-any.whl (242 kB)\n",
      "Downloading aiohttp-3.10.10-cp312-cp312-macosx_10_9_x86_64.whl (395 kB)\n",
      "Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading sentry_sdk-2.17.0-py2.py3-none-any.whl (314 kB)\n",
      "Downloading yarl-1.16.0-cp312-cp312-macosx_10_13_x86_64.whl (93 kB)\n",
      "Downloading gql-3.5.0-py2.py3-none-any.whl (74 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl (183 kB)\n",
      "Downloading setproctitle-1.3.3-cp312-cp312-macosx_10_9_x86_64.whl (11 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-macosx_10_13_x86_64.whl (54 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-macosx_10_9_x86_64.whl (29 kB)\n",
      "Downloading propcache-0.2.0-cp312-cp312-macosx_10_13_x86_64.whl (46 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: uuid-utils, tenacity, sniffio, smmap, setproctitle, sentry-sdk, pyyaml, propcache, multidict, mdurl, graphql-core, frozenlist, emoji, docker-pycreds, backoff, attrs, aiohappyeyeballs, yarl, requests-toolbelt, markdown-it-py, gitdb, anyio, aiosignal, rich, gql, gitpython, aiohttp, wandb, weave\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 anyio-4.6.2.post1 attrs-24.2.0 backoff-2.2.1 docker-pycreds-0.4.0 emoji-2.14.0 frozenlist-1.5.0 gitdb-4.0.11 gitpython-3.1.43 gql-3.5.0 graphql-core-3.2.5 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 propcache-0.2.0 pyyaml-6.0.2 requests-toolbelt-1.0.0 rich-13.9.3 sentry-sdk-2.17.0 setproctitle-1.3.3 smmap-5.0.1 sniffio-1.3.1 tenacity-9.0.0 uuid-utils-0.9.0 wandb-0.18.5 weave-0.51.17 yarl-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in as Weights & Biases user: 100386322.\n",
      "View Weave data at https://wandb.ai/100386322/weave-example/weave\n",
      "üç© https://wandb.ai/100386322/weave-example/r/call/0192c59c-d50b-7451-835b-c1f0fb99adfb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "weave.init(\"weave-example\")\n",
    "\n",
    "@weave.op()\n",
    "def sum_nine(value_one: int):\n",
    "    return value_one + 9\n",
    "\n",
    "@weave.op()\n",
    "def multiply_two(value_two: int):\n",
    "    return value_two * 2\n",
    "\n",
    "@weave.op()\n",
    "def main():\n",
    "    output = sum_nine(3)\n",
    "    final_output = multiply_two(output)\n",
    "    return final_output\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected model variable 'GenerativeModel': <class 'vertexai.generative_models.GenerativeModel'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "candidates {\n",
       "  content {\n",
       "    role: \"model\"\n",
       "    parts {\n",
       "      text: \"Please provide me with the `curl` output.  I need the HTML or text content you received from the `curl` command to transform it into structured markdown.  The more context you provide, the better I can do the job.  For example, tell me:\\n\\n* **The URL:** Where did the data come from? This helps me understand the context and intended structure.\\n* **The desired level of detail:** Do you want a simple markdown representation, or a highly structured one with headings, lists, tables, and code blocks?\\n* **Specific elements to include or exclude:** Are there specific parts of the webpage you want me to focus on or ignore?\\n* **Handling of images and links:** How should I handle images and links found in the webpage (e.g., should I preserve them, replace them with descriptions, or omit them entirely)?\\n\\nThe more information you give me, the more accurate and useful the resulting markdown will be.  A poorly formatted or incomplete input will result in a less accurate output.\\n\"\n",
       "    }\n",
       "  }\n",
       "  finish_reason: STOP\n",
       "  avg_logprobs: -0.25087255801794661\n",
       "}\n",
       "usage_metadata {\n",
       "  prompt_token_count: 35\n",
       "  candidates_token_count: 212\n",
       "  total_token_count: 247\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import inspect\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "\n",
    "# Instantiate the actual model from vertexai\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-flash-002\",  # Replace with your desired Gemini model ID\n",
    "    system_instruction=\"You are an expert in transforming the content of webpages into structured markdown. You just receive the result from curl and you need to transform it into markdown.\",\n",
    ")\n",
    "\n",
    "# Minimal decorator to identify and print the model dynamically\n",
    "def find_model_decorator(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Execute the function\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        # Get the current frame\n",
    "        frame = inspect.currentframe()\n",
    "        try:\n",
    "            # Get the locals from the frame where the decorated function was called\n",
    "            locals_after = frame.f_back.f_locals\n",
    "\n",
    "            # Find and print the model object based on attribute\n",
    "            for var_name, var_value in locals_after.items():\n",
    "                if callable(getattr(var_value, \"generate_content\", None)):\n",
    "                    print(f\"Detected model variable '{var_name}': {var_value}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"No model with 'generate_content' method found.\")\n",
    "        finally:\n",
    "            del frame  # Avoid reference cycles\n",
    "\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Function that uses the model\n",
    "@find_model_decorator\n",
    "def main(page_content: str):\n",
    "    response = model.generate_content(page_content)\n",
    "    return response\n",
    "\n",
    "# Test function\n",
    "main(\"Sample prompt for model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_MODEL_ROLE', 'model'), ('_USER_ROLE', 'user'), ('_cached_content', None), ('_generate_content', <bound method _GenerativeModel._generate_content of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('_generate_content_async', <bound method _GenerativeModel._generate_content_async of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('_generate_content_streaming', <bound method _GenerativeModel._generate_content_streaming of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('_generate_content_streaming_async', <bound method _GenerativeModel._generate_content_streaming_async of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('_generation_config', None), ('_llm_utility_async_client', <google.cloud.aiplatform_v1beta1.services.llm_utility_service.async_client.LlmUtilityServiceAsyncClient object at 0x11f76a3f0>), ('_llm_utility_async_client_value', <google.cloud.aiplatform_v1beta1.services.llm_utility_service.async_client.LlmUtilityServiceAsyncClient object at 0x11f76a3f0>), ('_llm_utility_client', <google.cloud.aiplatform_v1beta1.services.llm_utility_service.client.LlmUtilityServiceClient object at 0x11f769ca0>), ('_llm_utility_client_value', <google.cloud.aiplatform_v1beta1.services.llm_utility_service.client.LlmUtilityServiceClient object at 0x11f769ca0>), ('_location', 'us-central1'), ('_model_name', 'publishers/google/models/gemini-1.5-flash-002'), ('_parse_response', <bound method _GenerativeModel._parse_response of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('_prediction_async_client', <google.cloud.aiplatform_v1beta1.services.prediction_service.async_client.PredictionServiceAsyncClient object at 0x11f768aa0>), ('_prediction_async_client_value', <google.cloud.aiplatform_v1beta1.services.prediction_service.async_client.PredictionServiceAsyncClient object at 0x11f768aa0>), ('_prediction_client', <google.cloud.aiplatform_v1beta1.services.prediction_service.client.PredictionServiceClient object at 0x11e91d2b0>), ('_prediction_client_value', <google.cloud.aiplatform_v1beta1.services.prediction_service.client.PredictionServiceClient object at 0x11e91d2b0>), ('_prediction_resource_name', 'projects/mm-cia-dev/locations/us-central1/publishers/google/models/gemini-1.5-flash-002'), ('_prepare_request', <bound method _GenerativeModel._prepare_request of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('_safety_settings', None), ('_system_instruction', 'You are an expert in transforming the content of webpages into structured markdown. You just receive the result from curl and you need to transform it into markdown.'), ('_tool_config', None), ('_tools', None), ('compute_tokens', <bound method _GenerativeModel.compute_tokens of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('compute_tokens_async', <bound method _GenerativeModel.compute_tokens_async of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('count_tokens', <bound method _GenerativeModel.count_tokens of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('count_tokens_async', <bound method _GenerativeModel.count_tokens_async of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('generate_content', <bound method _GenerativeModel.generate_content of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('generate_content_async', <bound method _GenerativeModel.generate_content_async of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>), ('start_chat', <bound method _GenerativeModel.start_chat of <vertexai.generative_models.GenerativeModel object at 0x11f6c9d00>>)]\n"
     ]
    }
   ],
   "source": [
    "print([(a, getattr(model, a)) for a in dir(model) if not a.startswith(\"__\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vertexai.generative_models.GenerativeModel at 0x11f6c9d00>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prices': [{'name': 'skus/D5F5-4280-60ED/price', 'currencyCode': 'USD', 'valueType': 'rate', 'rate': {'tiers': [{'startAmount': {'value': '0'}, 'listPrice': {'currencyCode': 'USD', 'nanos': 7812}}], 'unitInfo': {'unit': 'count', 'unitDescription': 'count', 'unitQuantity': {'value': '1'}}, 'aggregationInfo': {'level': 'LEVEL_ACCOUNT', 'interval': 'INTERVAL_MONTHLY'}}}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from google.auth import default\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# Get the access token from the default application credentials\n",
    "def get_access_token():\n",
    "    credentials, _ = default()\n",
    "    credentials.refresh(Request())\n",
    "    return credentials.token\n",
    "\n",
    "def get_sku_pricing(service_id):\n",
    "    access_token = get_access_token()\n",
    "    url = f\"https://cloudbilling.googleapis.com/v1beta/skus/{service_id}/prices\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(response.json())\n",
    "        return response.json()  # Returns the JSON response\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Example service_id for Compute Engine\n",
    "service_id = \"D5F5-4280-60ED\"\n",
    "pricing_info = get_sku_pricing(service_id)\n",
    "\n",
    "if pricing_info:\n",
    "    # Print the SKU names and prices\n",
    "    for sku in pricing_info.get(\"skus\", []):\n",
    "        print(\"SKU Name:\", sku.get(\"description\"))\n",
    "        print(\"Pricing Info:\", sku.get(\"pricingInfo\", []))\n",
    "        print(\"-\" * 20)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
