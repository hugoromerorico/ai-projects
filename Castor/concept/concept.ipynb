{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concept/concept.py\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    FunctionDeclaration,\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")\n",
    "import json\n",
    "\n",
    "# Initialize Vertex AI with your project ID and location\n",
    "vertexai.init(project=\"mm-cia-dev\", location=\"europe-west1\")  # Replace with your project ID and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the name of a sibling\n",
    "def get_sibling(sibling: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns the name of a sibling based on the type.\n",
    "    \"\"\"\n",
    "    if sibling.lower() == \"sister\":\n",
    "        return {\"result\": \"Mary\"}\n",
    "    elif sibling.lower() == \"brother\":\n",
    "        return {\"result\": \"John\"}\n",
    "    else:\n",
    "        return {\"result\": \"Unknown\"}\n",
    "\n",
    "get_sibling_func = FunctionDeclaration(\n",
    "    name=\"get_sibling\",\n",
    "    description=\"Returns the name of a sibling based on the type.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"sibling\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Type of sibling (e.g., sister, brother)\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"sibling\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "sibling_tool = Tool(\n",
    "    function_declarations=[get_sibling_func],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      function_call {\n",
      "        name: \"get_sibling\"\n",
      "        args {\n",
      "          fields {\n",
      "            key: \"sibling\"\n",
      "            value {\n",
      "              string_value: \"sister\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  avg_logprobs: -9.3464157544076449e-06\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 95\n",
      "  candidates_token_count: 5\n",
      "  total_token_count: 100\n",
      "}\n",
      "\n",
      "Function Call Detected:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type FunctionCall is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m function_call \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfunction_calls[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction Call Detected:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoc/lib/python3.12/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoc/lib/python3.12/json/encoder.py:202\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoc/lib/python3.12/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/autodoc/lib/python3.12/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type FunctionCall is not JSON serializable"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the tool that includes the function declaration\n",
    "# Define the user's prompt as a Content object\n",
    "user_prompt_content = \"Can you give me the name of my sister?\"\n",
    "\n",
    "\n",
    "# Initialize the Gemini model with the specified tools\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-flash-002\",  # Replace with your desired Gemini model ID\n",
    "    system_instruction=\"You are a helpful assistant that can answer questions and help with tasks. You have tools to get the name of a sibling. If the user ask for a name you can use a tool to get either the name of the brother or the sister, depending on what the user ask for.\",\n",
    "    generation_config=GenerationConfig(temperature=0.5),\n",
    "    tools=[sibling_tool],\n",
    ")\n",
    "\n",
    "# Generate content with the user prompt\n",
    "response = model.generate_content(\n",
    "    user_prompt_content,\n",
    ")\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "# Extract the function call from the model's response\n",
    "function_call = response.candidates[0].function_calls[0]\n",
    "\n",
    "print(\"Function Call Detected:\")\n",
    "print(json.dumps(function_call, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "# Google Cloud Gemini Hackathon\n",
      "\n",
      "**This hackathon is open to registrants from Europe, Middle East and Africa (specific countries).**\n",
      "\n",
      "## Overview\n",
      "\n",
      "Welcome to the Google Cloud Gemini Hackathon! Join us for an exciting hackathon where creativity meets cutting-edge technology. We invite developers, innovators, and tech enthusiasts to come together and build groundbreaking applications using the Gemini API, in either Google AI Studio or Vertex AI environment. This is your chance to create solutions that not only showcase your skills but also address some of the most pressing challenges in the modern world.\n",
      "\n",
      "\n",
      "**Join Hackathon:** [https://googlecloudgeminihackathon.devpost.com/register?flow%5Bdata%5D%5Bchallenge_id%5D=22241&amp;flow%5Bname%5D=register_for_challenge](https://googlecloudgeminihackathon.devpost.com/register?flow%5Bdata%5D%5Bchallenge_id%5D=22241&amp;flow%5Bname%5D=register_for_challenge)\n",
      "\n",
      "**Deadline:** Nov 11, 2024 @ 5:00pm GMT\n",
      "\n",
      "\n",
      "### Eligibility\n",
      "\n",
      "*   Ages 18+ only\n",
      "*   Only specific countries/territories included: Algeria, Angola, Austria, Bahrain, Belgium, Benin, Botswana, Bulgaria, Burkina Faso, Burundi, Cameroon, Central African Republic, Chad, Comoros, Congo the Democratic Republic of the, Croatia, Czech Republic, Denmark, Djibouti, Egypt, Equatorial Guinea, Eritrea, Estonia, Ethiopia, Finland, France, Gabon, Gambia, Georgia, Germany, Ghana, Gibraltar, Greece, Guernsey, Guinea, Guinea-Bissau, Hungary, Iceland, Iraq, Ireland, Isle of Man, Israel, Italy, Jersey, Jordan, Kenya, Kuwait, Latvia, Lebanon, Lesotho, Liberia, Libyan Arab Jamahiriya, Liechtenstein, Lithuania, Luxembourg, Madagascar, Malawi, Mali, Malta, Mauritania, Mauritius, Morocco, Mozambique, Namibia, Netherlands, Niger, Nigeria, Norway, Oman, Palestine State of, Poland, Portugal, Qatar, Romania, Rwanda, Sao Tome and Principe, Saudi Arabia, Senegal, Slovakia, Slovenia, Somalia, South Africa, Spain, Swaziland, Sweden, Switzerland, Tanzania United Republic of, Togo, Tunisia, Turkey, Uganda, United Arab Emirates, United Kingdom, Western Sahara, Yemen, Zambia, Zimbabwe\n",
      "\n",
      "View full rules: [/rules](/rules)\n",
      "\n",
      "\n",
      "## Requirements\n",
      "\n",
      "### What to Build\n",
      "\n",
      "This hackathon is your playground to create or modify an application that leverages the power of the Gemini API, in either the Google AI Studio or Vertex AI environment. Whether you're focused on Security, Sovereignty, Sustainability, or the Smart Use of Data and AI, the possibilities are endless. Your innovation could be the next big thing in:\n",
      "\n",
      "1.  **Security**: Develop solutions like Cloud Workload Protection, Phishing Detection, or a Threat Intelligence Platform to safeguard our digital world.\n",
      "2.  **Sovereignty**: Create tools that facilitate cybersecurity cooperation between nations, assess data sovereignty risks, or ensure compliance with data localization regulations.\n",
      "3.  **Sustainability**: Design apps that promote sustainable sourcing, integrate renewable energy, or automate energy audits to drive a greener future.\n",
      "4.  **Smart Use of Data & AI**: Innovate with AI Ethics Frameworks, customer service chatbots, explainable AI systems, or predictive analytics for business forecasting.\n",
      "\n",
      "\n",
      "### What to Submit\n",
      "\n",
      "*   Project built with Google Gemini API, in either Google AI Studio, or Vertex AI environment\n",
      "*   Public link to your project for judges to test\n",
      "*   Google Project ID\n",
      "*   Public code repository showing use of Gemini product within your idea\n",
      "*   Text description that includes a summary of the Project’s features and functionality\n",
      "*   ~3-minute demonstration video of your Project\n",
      "\n",
      "\n",
      "## Prizes\n",
      "\n",
      "**$20,000** in prizes\n",
      "\n",
      "**1st Place:** $7,500 in USD, 30-minute Virtual Meeting with a Google Team Member, Social Promo of the Project\n",
      "\n",
      "**2nd Place:** $5,000 in USD, 30-minute Virtual Meeting with a Google Team Member, Social Promo of the Project\n",
      "\n",
      "**3rd Place:** $2,500 in USD, 30-minute Virtual Meeting with a Google Team Member, Social Promo of the Project\n",
      "\n",
      "**Honorable Mentions (5):** $1,000 in USD, 30-minute Virtual Meeting with a Google Team Member, Social Promo of the Project\n",
      "\n",
      "\n",
      "## Judging Criteria\n",
      "\n",
      "*   **Technological Implementation:** Does the interaction with the Gemini API demonstrate quality software development?\n",
      "*   **Design:** Is the user experience and design of the project well thought out?\n",
      "*   **Potential Impact:** How big of an impact could the project have on the Security, Sovereignty, Sustainability or Data and AI sectors?\n",
      "*   **Quality of Idea:** How creative and unique is the project?\n",
      "\n",
      "\n",
      "## Judges\n",
      "\n",
      "*   **Benjamin Sadik:** *Customer Engineer*\n",
      "*   **Catayoun Azarm:** *Customer Engineer*\n",
      "*   **Eloise de Carvalho:** *Customer Engineer*\n",
      "*   **Kimoon Kim:** *Customer Engineer*\n",
      "*   **Marco Acunzo:** *Customer Engineer*\n",
      "\n",
      "\n",
      "## Sponsors\n",
      "\n",
      "[Google Cloud](https://s3.amazonaws.com/challengepost/sponsors/logos/000/037/005/highres/logo_cloud_color.png)\n",
      "\n",
      "\n",
      "## Contact\n",
      "\n",
      "Questions? Email the hackathon manager: shawni@devpost.com\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-flash-002\",  # Replace with your desired Gemini model ID\n",
    "    system_instruction=\"You are an expert in transforming the content of webpages into structured markdown. You just receive the result from curl and you need to transform it into markdown.\",\n",
    "    generation_config=GenerationConfig(temperature=0.5),\n",
    ")\n",
    "# Curl \"https://googlecloudgeminihackathon.devpost.com/?ref_feature=challenge&ref_medium=discover\"\n",
    "import requests\n",
    "\n",
    "# Fetch content from the URL\n",
    "url = \"https://googlecloudgeminihackathon.devpost.com/?ref_feature=challenge&ref_medium=discover\"\n",
    "page_content = requests.get(url).text\n",
    "\n",
    "\n",
    "# Generate content with the user prompt\n",
    "response = model.generate_content(\n",
    "    page_content,\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Execute the function based on the model's function call\n",
    "if function_call.name == function_name:\n",
    "    sibling_type = function_call.args[\"sibling\"]\n",
    "    function_result = get_sibling(sibling_type)\n",
    "    print(\"\\nFunction Execution Result:\")\n",
    "    print(json.dumps(function_result, indent=4))\n",
    "\n",
    "    # Prepare the function response as Content\n",
    "    function_response_content = Content(\n",
    "        role=\"function\",\n",
    "        name=function_name,\n",
    "        parts=[\n",
    "            Part.from_function_response(\n",
    "                name=function_name,\n",
    "                response=function_result\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Generate the final model response with the function result\n",
    "    final_response = model.generate_content(\n",
    "        [\n",
    "            user_prompt_content,          # User prompt\n",
    "            response.candidates[0].content,  # Function call response\n",
    "            function_response_content     # Function execution result\n",
    "        ],\n",
    "        tools=[sibling_tool],\n",
    "    )\n",
    "\n",
    "    print(\"\\nFinal Model Response:\")\n",
    "    print(final_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
